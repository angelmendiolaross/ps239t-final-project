---
title: "Merging 1992-2017 Police Data with 2018 Data"
author: "Angel Mendiola Ross"
date: "5/6/2020"
output: html_document
---

#1.0 Setup

```{r setup}
knitr::opts_knit$set(root.dir = '/Users/angelmr/ps239t-final-project/')
```

```{r}

### Clear global environment
rm(list=ls())

# Set working directory
getwd()

# install.packages("readxl") # Needed this for reading in xlsx files which is how the state provides raw data
# install.packages("DataCombine") # Needed this for the slide function to lad/lead variables
library(pacman)
pacman::p_unload(all)

pacman::p_load(
  tidyverse, #dplyr, readr, etc.
  plyr, #for editing factor levels
  magrittr, #%<>% operator
  readxl, #for excel workbooks
  tidyr, #to transform from wide to long
  ggplot2, #for plotting
  DataCombine, # for lagging and leading variables
  ggthemes # for pretty charts
)
```

## 1.1 Joining 1992-2002 Policing Data with 2003-2018 data

Note: The 2003-2018 data came with a lot more variables (e.g. federal transfers, community development, housing, transportation expenditures, etc.). The 1992-2017 data only has police expenditures.

```{r, echo=F}

# load data
load("data/policing_data_9217.RData")
load("data/city_fin_data.RData")

# looking at the variables 
head(city_fin_data) # dataset has 23 columns
head(police_long) # dataset has 4 columns

```

Now that I have the two files loaded, I want to strip down the city_fin_data to the variables I will be focusing on in the anaysis then add the appropriate columns to the police_long data to prepare for rbind.

``` {r}

# stripping down city_fin_data to just policing data for 2018 (it seems that the police_long dataset is more complete)
police_18 <- city_fin_data %>%
  subset(year==2018) %>%
  select(year,city_name,police_total)

# renaming city variable
police_long %<>% dplyr::rename(city_name = City)

# removing county variable
police_long <- police_long[-c(3)]

```

Now I am ready to combine the datasets with an rbind.

``` {r}
# getting rid of columns not going into my analysis of city police spending
city_fin_small <- city_fin_data[c(2,1,3:5,7,13,16,17,19,22)]

names(city_fin_small) # now city_fin_small is down to 11 vars and ready to join with police data later

# creating a new data frame with the complete data
policing_complete <- rbind(police_long, police_18)

# now joining with the rest of the city financial data
policing_complete <- left_join(x=policing_complete,
                           y=city_fin_small,
                           by=c("year", "city_name"))

# saving complete policing data
save(policing_complete,
     file = "data/policing_complete.RData")

```

## 1.2 Looking at the combined dataset

``` {r viewing combined data}

# assigning police_total as numerical
policing_complete$police_total <- as.numeric(as.character(policing_complete$police_total))

# plotting the policing variable
ggplot(policing_complete, aes(x = year, y = police_total)) +
  geom_point() +
  geom_smooth(method = "lm", color = "orange") +
  scale_y_log10() +
  xlim(1990, 2020) +
  ggtitle("Policing Spending of California Cities") +
  labs(y="Police Spending (logged)", x = "Year") +
  theme_economist() + 
  scale_colour_economist()

# Save plot 
ggsave(filename="plots/police_spending.pdf")

# looking at the mean and median
policing_complete %>%
  dplyr::group_by(year) %>%
  dplyr::summarise(mean_police = mean(police_total, na.rm = T),
            median_police = median(police_total, na.rm = T))

```

## 1.3 Adjusting my main varables of interest for inflation

I need to adjust both police spending (dependent variable) and federal transfers (independent variable) for inflation.

The BLS has an API but I can only call Consumer Price Index (CPI) values for 10 years at time. This is not ideal because I have data across 26 years.

But I found a California-specific CPI (even better because my analysis is CA-focused) from the State's EDD at the following website: <https://www.labormarketinfo.edd.ca.gov/cgi/databrowsing/localAreaProfileQSMoreResult.asp?viewAll=&viewAllUS=&currentPage=&currentPageUS=&sortUp=&sortDown=Year&criteria=consumer+price+index&categoryType=economicindicators&geogArea=0601000000&timeseries=consumer+price+indexTimeSeries&more=&menuChoice=localAreaPro&printerFriendly=&BackHistory=-1&goTOPageText=&USgoTOPage=>

I downloaded the data, which comes with California and US CPI values and created a second sheet in the file with just the California data.

``` {r, echo = F}

# get data from the excel file
cpi <- read_excel("data/raw/consumer_price_index.xlsx", sheet = "ca_clean")

# rename Year to year for join
cpi %<>% dplyr::rename(year = Year)

# join cpi to the policing data
policing_complete <- left_join(policing_complete, cpi, by = "year")

# rename cpi column to base_cpi
policing_complete %<>% dplyr::rename(base_cpi = CPI)

# dropping unneeded columns
policing_complete <- policing_complete[c(1:12,14)]

# checking 2018 CPI value
policing_complete %>%
  subset(year==2018) %>%
  select(base_cpi)
# 272.5

# adding a new_cpi column to reflect 2018 CPI: 272.5
policing_complete %<>%
  mutate(new_cpi = 272.5)

# now calculating the inflation-adjusted values of police_total, which is (nominal_value) * (new_cpi/base_cpi)
policing_complete %<>%
  mutate(police_infl_adj = NA) %>%
  mutate(police_infl_adj = ifelse(!is.na(police_total), police_total * (new_cpi/base_cpi), police_infl_adj))

# checking a random sample to make sure it looks right
policing_complete %>% 
  select(year,city_name,police_total,police_infl_adj) %>%
  sample_n(size = 10)

```

Now I'd like to plot the main dependent variable (the natural logarithm of inflation-adjusted policing spending) and save my plot.

``` {r, echo=F}
# plotting the inflation-adjusted policing variable
ggplot(policing_complete, aes(x = year, y = police_infl_adj)) +
  geom_point() +
  geom_smooth(method = "lm", color = "orange") +
  scale_y_log10() +
  xlim(1990, 2020) +
  ggtitle("Inflation-Adjusted Policing Spending of California Cities") +
  labs(y="Inflation-Adjusted Police Spending (logged)", x = "Year") +
  theme_economist() + 
  scale_colour_economist()

# Save plot 
ggsave(filename="plots/infl_adj_police_spending.pdf")
ggsave(filename="results/infl_adj_police_spending.pdf")
```
We already see that adjusting for inflation (via the California CPI) leads to a much less pronounced average increase in police spending from 1992 to 2018. Let's check summary statistics.

```{r}
# checking change over time for summary variables
policing_complete %>%
  dplyr::group_by(year) %>%
  dplyr::summarise(mean_police = mean(police_total, na.rm = T),
            median_police = median(police_total, na.rm = T),
            mean_police_adj = mean(police_infl_adj, na.rm = T),
            median_police_adj = median(police_infl_adj, na.rm = T))

# inflation-adjusted median was $5,929,492 in 2000 and $8,536,630 in 2018 (still increased)
```

The other variable I should adjust for inflation is federal transfers, one of my control variables following Vargas and McHarris (2017).

```{r}
# formula is (nominal_value) * (new_cpi/base_cpi)
policing_complete %<>%
  mutate(fed_transfers_infl_adj = NA) %>%
  mutate(fed_transfers_infl_adj = ifelse(!is.na(total_fed_rev), total_fed_rev * (new_cpi/base_cpi), fed_transfers_infl_adj))

# checking a random sample to make sure it looks right
policing_complete %>% 
  select(year,city_name,total_fed_rev,fed_transfers_infl_adj) %>%
  sample_n(size = 10)

```

## 1.4 Now that I have my main dependent variable (and one of my controls) ready to go, I just need to create a leaded (t+1 & t+2) version of my dependent variable

``` {r}
# first I need to make sure R understands the ordering of my years (not having this step created an issue)
dates <- unique(sort(policing_complete$year))
policing_complete$year <- factor(policing_complete$year, labels = dates,  ordered = T)

policing_complete %>% nrow() #rows in dataset=12928
policing_complete %>% distinct() %>% nrow() #unique rows in dataset=12928

# leading the inflation-adjusted policing measure by 1 year using slide function
policing_complete_test <- slide(policing_complete, Var = "police_infl_adj", TimeVar = "year", GroupVar = "city_name", NewVar = "police_infl_lead", slideBy = 1, keepInvalid = TRUE)

# checking a few cities to make sure it looks right
policing_complete_test %>%
  subset(year >= 2008 & year <= 2018 & city_name == "Barstow") %>%
  select(year,city_name,police_infl_adj,police_infl_lead)

policing_complete_test %>% 
  subset(year >= 2006 & year <= 2018 & city_name == "Portola Valley") %>%
  select(year,city_name,police_infl_adj,police_infl_lead)

policing_complete_test %>% 
  subset(year >= 2006 & year <= 2018 & city_name == "Corona") %>%
  select(year,city_name,police_infl_adj,police_infl_lead)

# leading two years to also test if it makes a difference
policing_complete_lead <- slide(policing_complete_test, Var = "police_infl_adj", TimeVar = "year", GroupVar = "city_name", NewVar = "police_infl_lead_2", slideBy = 2, keepInvalid = TRUE)

# checking some random cities
policing_complete_lead %>% 
  subset(year >= 2009 & year <= 2018 & city_name == "Temecula") %>%
  select(year,city_name,police_infl_adj,police_infl_lead,police_infl_lead_2)

# saving this cleaned file
save(policing_complete_lead,
     file = "data/policing_complete_lead.RData")

```

